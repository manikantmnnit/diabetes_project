{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy dataset\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader,Dataset, dataloader,random_split\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-19 17:05:09--  https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23105 (23K) [text/plain]\n",
      "Saving to: â€˜diabetes.csv.1â€™\n",
      "\n",
      "diabetes.csv.1      100%[===================>]  22.56K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-19 17:05:09 (92.7 MB/s) - â€˜diabetes.csv.1â€™ saved [23105/23105]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import stack_size\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiabeticDataset(Dataset):\n",
    "    X:torch.Tensor\n",
    "    y:torch.Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "\n",
    "# normalization\n",
    "class Normalization_dataset(Dataset):\n",
    "    def __init__(self, base_dataset, mean, std):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        # ðŸ”¥ preserve indices if base_dataset is a Subset\n",
    "        if hasattr(base_dataset, \"indices\"):\n",
    "            self.indices = base_dataset.indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.base_dataset[idx]\n",
    "        X = (X - self.mean) / (self.std + 1e-8)\n",
    "        return X, y\n",
    "\n",
    "class DiabeticDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        batch_size=16,\n",
    "        train_ratio=0.75,\n",
    "        seed=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df=df\n",
    "        self.batch_size=batch_size\n",
    "        self.train_ratio=train_ratio\n",
    "        self.seed=seed\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        X=df.drop(columns='Outcome',axis=1).values\n",
    "        y=df['Outcome'].values\n",
    "\n",
    "        # convert into tensor\n",
    "        X=torch.tensor(X,dtype=torch.float32)\n",
    "        y=torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "        full_dataset=DiabeticDataset(X,y)\n",
    "\n",
    "        train_size = int(self.train_ratio * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        generator=torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        self.train_ds, self.test_ds = random_split(\n",
    "            full_dataset,\n",
    "            [train_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def normalize_datasets(self):\n",
    "        X_all = []\n",
    "\n",
    "        for X, y in self.train_dataloader():\n",
    "            X_all.append(X.cpu())\n",
    "\n",
    "        X_all = torch.cat(X_all, dim=0)\n",
    "\n",
    "        mean = X_all.mean(dim=0)\n",
    "        std  = X_all.std(dim=0)\n",
    "\n",
    "        # Wrap datasets\n",
    "        self.train_ds = Normalization_dataset(self.train_ds, mean, std)\n",
    "        self.test_ds  = Normalization_dataset(self.test_ds,  mean, std)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([  3.8264, 121.0104,  69.0521,  20.2465,  78.5729,  31.9856,   0.4695,\n",
      "         33.1042])\n",
      "Std: tensor([  3.4117,  32.4606,  19.7235,  16.0761, 112.1771,   7.9008,   0.3158,\n",
      "         11.5159])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "dm = DiabeticDataModule(df=df, seed=36)\n",
    "dm.setup()\n",
    "\n",
    "mean, std = dm.normalize_datasets()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader  = dm.test_dataloader()\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect train data\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for x, y in train_loader.dataset:\n",
    "    X_train_list.append(x)\n",
    "    y_train_list.append(y)\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0)  # (N_train, num_features)\n",
    "y_train = torch.tensor(y_train_list)         # (N_train,)\n",
    "\n",
    "# Collect test data\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for x, y in test_loader.dataset:\n",
    "    X_test_list.append(x)\n",
    "    y_test_list.append(y)\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0)   # (N_test, num_features)\n",
    "y_test = torch.tensor(y_test_list)    \n",
    "\n",
    "from pathlib import Path\n",
    "save_dir=Path.cwd().parent/'data'/'splits'\n",
    "save_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# File path\n",
    "save_path = save_dir / \"diabetes_normalized.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save split data inot csv and store in dvc\n",
    "train_indices=dm.train_ds.indices\n",
    "test_indices=dm.test_ds.indices\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "data_dir=Path.cwd().parent/'data'\n",
    "# Create 'splits' folder inside 'data' directory\n",
    "splits_dir = data_dir / 'splits'\n",
    "\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.iloc[train_indices].to_csv(splits_dir / 'train.csv', index=False)\n",
    "df.iloc[test_indices].to_csv(splits_dir / 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  3.8264, 121.0104,  69.0521,  20.2465,  78.5729,  31.9856,   0.4695,\n",
       "          33.1042]),\n",
       " tensor([  3.4117,  32.4606,  19.7235,  16.0761, 112.1771,   7.9008,   0.3158,\n",
       "          11.5159]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the training and test dataset\n",
    "\n",
    "X_all=[]\n",
    "for X,y in train_loder:\n",
    "   \n",
    "    X_all.append(X)\n",
    "\n",
    "mean,std=torch.cat(X_all,0).mean(dim=0),torch.cat(X_all,0).std(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.84505208, 120.89453125,  69.10546875,  20.53645833,\n",
       "        79.79947917,  31.99257812,   0.4718763 ,  33.24088542])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,:-1]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar=StandardScaler()\n",
    "scalar.fit(df.iloc[:,:-1])\n",
    "scalar.mean_,scalar.st"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
