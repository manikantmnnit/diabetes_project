{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy dataset\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader,Dataset, dataloader,random_split\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import dagshub\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-21 14:19:39--  https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23105 (23K) [text/plain]\n",
      "Saving to: â€˜diabetes.csv.3â€™\n",
      "\n",
      "diabetes.csv.3      100%[===================>]  22.56K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-21 14:19:39 (89.5 MB/s) - â€˜diabetes.csv.3â€™ saved [23105/23105]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import stack_size\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiabeticDataset(Dataset):\n",
    "    X:torch.Tensor\n",
    "    y:torch.Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "\n",
    "# normalization\n",
    "class Normalization_dataset(Dataset):\n",
    "    def __init__(self, base_dataset, mean, std):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        # ğŸ”¥ preserve indices if base_dataset is a Subset\n",
    "        if hasattr(base_dataset, \"indices\"):\n",
    "            self.indices = base_dataset.indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.base_dataset[idx]\n",
    "        X = (X - self.mean) / (self.std + 1e-8)\n",
    "        return X, y\n",
    "\n",
    "class DiabeticDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        batch_size=16,\n",
    "        train_ratio=0.75,\n",
    "        seed=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df=df\n",
    "        self.batch_size=batch_size\n",
    "        self.train_ratio=train_ratio\n",
    "        self.seed=seed\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        X=df.drop(columns='Outcome',axis=1).values\n",
    "        y=df['Outcome'].values\n",
    "\n",
    "        # convert into tensor\n",
    "        X=torch.tensor(X,dtype=torch.float32)\n",
    "        y=torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "        full_dataset=DiabeticDataset(X,y)\n",
    "\n",
    "        train_size = int(self.train_ratio * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        generator=torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        self.train_ds, self.test_ds = random_split(\n",
    "            full_dataset,\n",
    "            [train_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def normalize_datasets(self):\n",
    "        X_all = []\n",
    "\n",
    "        for X, y in self.train_dataloader():\n",
    "            X_all.append(X.cpu())\n",
    "\n",
    "        X_all = torch.cat(X_all, dim=0)\n",
    "\n",
    "        mean = X_all.mean(dim=0)\n",
    "        std  = X_all.std(dim=0)\n",
    "\n",
    "        # Wrap datasets\n",
    "        self.train_ds = Normalization_dataset(self.train_ds, mean, std)\n",
    "        self.test_ds  = Normalization_dataset(self.test_ds,  mean, std)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([  3.8264, 121.0104,  69.0521,  20.2465,  78.5729,  31.9856,   0.4695,\n",
      "         33.1042])\n",
      "Std: tensor([  3.4117,  32.4606,  19.7235,  16.0761, 112.1771,   7.9008,   0.3158,\n",
      "         11.5159])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "dm = DiabeticDataModule(df=df, seed=36)\n",
    "dm.setup()\n",
    "\n",
    "mean, std = dm.normalize_datasets()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader  = dm.test_dataloader()\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect train data\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for x, y in train_loader.dataset:\n",
    "    X_train_list.append(x)\n",
    "    y_train_list.append(y)\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0)  # (N_train, num_features)\n",
    "y_train = torch.tensor(y_train_list)         # (N_train,)\n",
    "\n",
    "# Collect test data\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for x, y in test_loader.dataset:\n",
    "    X_test_list.append(x)\n",
    "    y_test_list.append(y)\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0)   # (N_test, num_features)\n",
    "y_test = torch.tensor(y_test_list)    \n",
    "\n",
    "from pathlib import Path\n",
    "save_dir=Path.cwd().parent/'data'/'splits'\n",
    "save_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# File path\n",
    "save_path = save_dir / \"diabetes_normalized.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save split data inot csv and store in dvc\n",
    "train_indices=dm.train_ds.indices\n",
    "test_indices=dm.test_ds.indices\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "data_dir=Path.cwd().parent/'data'\n",
    "# Create 'splits' folder inside 'data' directory\n",
    "splits_dir = data_dir / 'splits'\n",
    "\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.iloc[train_indices].to_csv(splits_dir / 'train.csv', index=False)\n",
    "df.iloc[test_indices].to_csv(splits_dir / 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic algo: logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic algo: logistic Algorithm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Logistic_RgressionModel(nn.Module):\n",
    "    def __init__(self, featur_dim):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(featur_dim,1)   # single output either 0 or 1\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# setup model , loss and optimizer\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "featur_dim=8\n",
    "\n",
    "model=Logistic_RgressionModel(featur_dim=featur_dim)\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct=0\n",
    "    total=0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.float().unsqueeze(1).to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        probs=torch.sigmoid(logits)\n",
    "        predicts=(probs>0.5).long()\n",
    "        correct += (predicts == y.long()).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "\n",
    "    return total_loss / len(loader),correct/total\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long().squeeze(1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as manikantmnnit\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as manikantmnnit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"manikantmnnit/diabetes_project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"manikantmnnit/diabetes_project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository manikantmnnit/diabetes_project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository manikantmnnit/diabetes_project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] | Loss: 0.6929 | Train Acc: 0.5347 | Test Acc: 0.5625\n",
      "Epoch [10/50] | Loss: 0.6232 | Train Acc: 0.6615 | Test Acc: 0.6667\n",
      "Epoch [15/50] | Loss: 0.5784 | Train Acc: 0.7240 | Test Acc: 0.6979\n",
      "Epoch [20/50] | Loss: 0.5487 | Train Acc: 0.7431 | Test Acc: 0.7396\n",
      "Epoch [25/50] | Loss: 0.5281 | Train Acc: 0.7587 | Test Acc: 0.7500\n",
      "Epoch [30/50] | Loss: 0.5138 | Train Acc: 0.7604 | Test Acc: 0.7604\n",
      "Epoch [35/50] | Loss: 0.5033 | Train Acc: 0.7656 | Test Acc: 0.7656\n",
      "Epoch [40/50] | Loss: 0.4960 | Train Acc: 0.7708 | Test Acc: 0.7604\n",
      "Epoch [45/50] | Loss: 0.4905 | Train Acc: 0.7795 | Test Acc: 0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/21 14:22:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50] | Loss: 0.4868 | Train Acc: 0.7778 | Test Acc: 0.7656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/21 14:22:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/01/21 14:22:21 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.23.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torchvision==0.23.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run log_reg_baseline at: https://dagshub.com/manikantmnnit/diabetes_project.mlflow/#/experiments/0/runs/5ba24e77b2fd4ac0a44c583f9f343a13\n",
      "ğŸ§ª View experiment at: https://dagshub.com/manikantmnnit/diabetes_project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='manikantmnnit', repo_name='diabetes_project', mlflow=True)\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/manikantmnnit/diabetes_project.mlflow')\n",
    "\n",
    "num_epochs = 50\n",
    "mlflow.set_experiment(\"diabetes_logistic_regression\")\n",
    "with mlflow.start_run(run_name='log_reg_baseline'):\n",
    "    mlflow.log_param(\"model\", \"logistic_regression\")\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param('Batch_size',num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss,train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        test_acc   = evaluate(model, test_loader, device)\n",
    "\n",
    "        # ---- Log metrics per epoch ----\n",
    "        mlflow.log_metric(\"train_log_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc, step=epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "                f\"Loss: {train_loss:.4f} | \"\n",
    "                f\"Train Acc: {train_acc:.4f} | \"\n",
    "                f\"Test Acc: {test_acc:.4f}\"\n",
    "            )\n",
    "    \n",
    "    # log model\n",
    "    mlflow.pytorch.log_model(model,artifact_path='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using AIM to track experiments, ,metrics and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] | Loss: 0.4840 | Train Acc: 0.7795 | Test Acc: 0.7760\n",
      "Epoch [10/50] | Loss: 0.4820 | Train Acc: 0.7778 | Test Acc: 0.7760\n",
      "Epoch [15/50] | Loss: 0.4804 | Train Acc: 0.7778 | Test Acc: 0.7708\n",
      "Epoch [20/50] | Loss: 0.4793 | Train Acc: 0.7778 | Test Acc: 0.7760\n",
      "Epoch [25/50] | Loss: 0.4786 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [30/50] | Loss: 0.4779 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [35/50] | Loss: 0.4775 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [40/50] | Loss: 0.4772 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [45/50] | Loss: 0.4769 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [50/50] | Loss: 0.4767 | Train Acc: 0.7812 | Test Acc: 0.7865\n"
     ]
    }
   ],
   "source": [
    "from aim import Run\n",
    "from aim.pytorch import track_params_dists\n",
    "\n",
    "run = Run(experiment='diabetes_logistic_regression')\n",
    "\n",
    "# log hyperparameters once\n",
    "run['model'] = 'logistic_regression_pytorch'\n",
    "run['learning_rate'] = lr\n",
    "run['epochs'] = num_epochs\n",
    "\n",
    "run['optimizer'] = 'Adam'\n",
    "run['loss_fn'] = 'BCEWithLogitsLoss'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train & Evaluate\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    # ---- Log metrics per epoch ----\n",
    "    run.track(train_loss, name=\"train_log_loss\", step=epoch)\n",
    "    run.track(train_acc, name=\"train_accuracy\", step=epoch)\n",
    "    run.track(test_acc, name=\"test_accuracy\", step=epoch)\n",
    "\n",
    "    # ---- Optional: track weight/bias distributions ----\n",
    "    track_params_dists(model, run=run)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "            f\"Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Test Acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "# Close Aim run\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Metrics from LightningModule\n",
    "\n",
    "\n",
    "\n",
    "class LogisticRegressionLightningModel(pl.LightningModule):\n",
    "    def __init__(self,input_dim,lr=0.001):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(input_dim,1)\n",
    "        self.lr=lr\n",
    "        \n",
    "        self.criterion=torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # saved hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)        # ensure batch dimension\n",
    "        return self.linear(x) \n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        X,y=batch\n",
    "        y=y.float().unsqueeze(1)\n",
    "        # output by model\n",
    "        logits=self(X)\n",
    "        loss=self.criterion(logits,y)\n",
    "\n",
    "        probs=torch.sigmoid(logits)\n",
    "        preds=(probs>0.5).float()\n",
    "        acc=(preds==y).float().mean()\n",
    "\n",
    "        self.log('train_loss',loss,prog_bar=True)\n",
    "        self.log('train_accuracy',acc,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        X,y=batch\n",
    "        y=y.float().unsqueeze(1)\n",
    "        # output by model\n",
    "        logits=self(X)\n",
    "        loss=self.criterion(logits,y)\n",
    "\n",
    "        probs=torch.sigmoid(logits)\n",
    "        preds=(probs>0.5).float()\n",
    "        acc=(preds==y).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | linear    | Linear            | 9      | train\n",
      "1 | criterion | BCEWithLogitsLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "9         Trainable params\n",
      "0         Non-trainable params\n",
      "9         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152ab452c8e24f80951bc608f0715d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d84ae5338b4c01a920be5091c480e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6822916865348816     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6908219456672668     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6822916865348816    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6908219456672668    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.6908219456672668, 'val_acc': 0.6822916865348816}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dm = DiabeticDataModule(df=df, batch_size=32)\n",
    "# model = LogisticRegressionLightningModel(input_dim=8, lr=0.01)\n",
    "\n",
    " # set up AIm and mlflow loggers in lightning\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "mlflow_logger=MLFlowLogger(\n",
    "    experiment_name=\"diabetes_logistic_regression\",\n",
    "    tracking_uri=\"file:./mlruns\" \n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    logger=[mlflow_logger],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "dm = DiabeticDataModule(df=df, batch_size=32)\n",
    "model = LogisticRegressionLightningModel(input_dim=8, lr=0.01)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.test(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
