{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy dataset\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader,Dataset, dataloader,random_split\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import dagshub\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-20 14:03:31--  https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23105 (23K) [text/plain]\n",
      "Saving to: ‚Äòdiabetes.csv.1‚Äô\n",
      "\n",
      "diabetes.csv.1      100%[===================>]  22.56K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-20 14:03:31 (85.1 MB/s) - ‚Äòdiabetes.csv.1‚Äô saved [23105/23105]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import stack_size\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiabeticDataset(Dataset):\n",
    "    X:torch.Tensor\n",
    "    y:torch.Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "\n",
    "# normalization\n",
    "class Normalization_dataset(Dataset):\n",
    "    def __init__(self, base_dataset, mean, std):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        # üî• preserve indices if base_dataset is a Subset\n",
    "        if hasattr(base_dataset, \"indices\"):\n",
    "            self.indices = base_dataset.indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.base_dataset[idx]\n",
    "        X = (X - self.mean) / (self.std + 1e-8)\n",
    "        return X, y\n",
    "\n",
    "class DiabeticDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        batch_size=16,\n",
    "        train_ratio=0.75,\n",
    "        seed=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df=df\n",
    "        self.batch_size=batch_size\n",
    "        self.train_ratio=train_ratio\n",
    "        self.seed=seed\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        X=df.drop(columns='Outcome',axis=1).values\n",
    "        y=df['Outcome'].values\n",
    "\n",
    "        # convert into tensor\n",
    "        X=torch.tensor(X,dtype=torch.float32)\n",
    "        y=torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "        full_dataset=DiabeticDataset(X,y)\n",
    "\n",
    "        train_size = int(self.train_ratio * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        generator=torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        self.train_ds, self.test_ds = random_split(\n",
    "            full_dataset,\n",
    "            [train_size, test_size],\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def normalize_datasets(self):\n",
    "        X_all = []\n",
    "\n",
    "        for X, y in self.train_dataloader():\n",
    "            X_all.append(X.cpu())\n",
    "\n",
    "        X_all = torch.cat(X_all, dim=0)\n",
    "\n",
    "        mean = X_all.mean(dim=0)\n",
    "        std  = X_all.std(dim=0)\n",
    "\n",
    "        # Wrap datasets\n",
    "        self.train_ds = Normalization_dataset(self.train_ds, mean, std)\n",
    "        self.test_ds  = Normalization_dataset(self.test_ds,  mean, std)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([  3.8264, 121.0104,  69.0521,  20.2465,  78.5729,  31.9856,   0.4695,\n",
      "         33.1042])\n",
      "Std: tensor([  3.4117,  32.4606,  19.7235,  16.0761, 112.1771,   7.9008,   0.3158,\n",
      "         11.5159])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "dm = DiabeticDataModule(df=df, seed=36)\n",
    "dm.setup()\n",
    "\n",
    "mean, std = dm.normalize_datasets()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader  = dm.test_dataloader()\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect train data\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for x, y in train_loader.dataset:\n",
    "    X_train_list.append(x)\n",
    "    y_train_list.append(y)\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0)  # (N_train, num_features)\n",
    "y_train = torch.tensor(y_train_list)         # (N_train,)\n",
    "\n",
    "# Collect test data\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for x, y in test_loader.dataset:\n",
    "    X_test_list.append(x)\n",
    "    y_test_list.append(y)\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0)   # (N_test, num_features)\n",
    "y_test = torch.tensor(y_test_list)    \n",
    "\n",
    "from pathlib import Path\n",
    "save_dir=Path.cwd().parent/'data'/'splits'\n",
    "save_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# File path\n",
    "save_path = save_dir / \"diabetes_normalized.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save split data inot csv and store in dvc\n",
    "train_indices=dm.train_ds.indices\n",
    "test_indices=dm.test_ds.indices\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "data_dir=Path.cwd().parent/'data'\n",
    "# Create 'splits' folder inside 'data' directory\n",
    "splits_dir = data_dir / 'splits'\n",
    "\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.iloc[train_indices].to_csv(splits_dir / 'train.csv', index=False)\n",
    "df.iloc[test_indices].to_csv(splits_dir / 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic algo: logistic Algorithm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Logistic_RgressionModel(nn.Module):\n",
    "    def __init__(self, featur_dim):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(featur_dim,1)   # single output either 0 or 1\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# setup model , loss and optimizer\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "featur_dim=8\n",
    "\n",
    "model=Logistic_RgressionModel(featur_dim=featur_dim)\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct=0\n",
    "    total=0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.float().unsqueeze(1).to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        probs=torch.sigmoid(logits)\n",
    "        predicts=(probs>0.5).long()\n",
    "        correct += (predicts == y.long()).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "\n",
    "    return total_loss / len(loader),correct/total\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long().squeeze(1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"manikantmnnit/diabetes_project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"manikantmnnit/diabetes_project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository manikantmnnit/diabetes_project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository manikantmnnit/diabetes_project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] | Loss: 0.4762 | Train Acc: 0.7795 | Test Acc: 0.7865\n",
      "Epoch [10/50] | Loss: 0.4762 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [15/50] | Loss: 0.4762 | Train Acc: 0.7795 | Test Acc: 0.7865\n",
      "Epoch [20/50] | Loss: 0.4761 | Train Acc: 0.7795 | Test Acc: 0.7865\n",
      "Epoch [25/50] | Loss: 0.4761 | Train Acc: 0.7795 | Test Acc: 0.7865\n",
      "Epoch [30/50] | Loss: 0.4762 | Train Acc: 0.7778 | Test Acc: 0.7865\n",
      "Epoch [35/50] | Loss: 0.4761 | Train Acc: 0.7795 | Test Acc: 0.7865\n",
      "Epoch [40/50] | Loss: 0.4762 | Train Acc: 0.7795 | Test Acc: 0.7917\n",
      "Epoch [45/50] | Loss: 0.4761 | Train Acc: 0.7795 | Test Acc: 0.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/20 15:58:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50] | Loss: 0.4761 | Train Acc: 0.7778 | Test Acc: 0.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/20 15:58:21 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/01/20 15:58:28 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.23.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torchvision==0.23.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run log_reg_baseline at: https://dagshub.com/manikantmnnit/diabetes_project.mlflow/#/experiments/0/runs/58a978e2af6a4981b542105ade116416\n",
      "üß™ View experiment at: https://dagshub.com/manikantmnnit/diabetes_project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='manikantmnnit', repo_name='diabetes_project', mlflow=True)\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/manikantmnnit/diabetes_project.mlflow')\n",
    "\n",
    "num_epochs = 50\n",
    "mlflow.set_experiment(\"diabetes_logistic_regression\")\n",
    "with mlflow.start_run(run_name='log_reg_baseline'):\n",
    "    mlflow.log_param(\"model\", \"logistic_regression\")\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param('Batch_size',num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss,train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        test_acc   = evaluate(model, test_loader, device)\n",
    "\n",
    "        # ---- Log metrics per epoch ----\n",
    "        mlflow.log_metric(\"train_log_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc, step=epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "                f\"Loss: {train_loss:.4f} | \"\n",
    "                f\"Train Acc: {train_acc:.4f} | \"\n",
    "                f\"Test Acc: {test_acc:.4f}\"\n",
    "            )\n",
    "    \n",
    "    # log model\n",
    "    mlflow.pytorch.log_model(model,artifact_path='model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
